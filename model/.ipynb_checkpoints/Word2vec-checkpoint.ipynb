{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yura/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/yura/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yura/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    " \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('/home/yura/PycharmProjects/news-filter/data/db/news_filter.db')\n",
    "\n",
    "data = pd.read_sql_query(\"SELECT * FROM article\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "\n",
    "data['category'] = data['category'].astype(int)\n",
    "\n",
    "train, test = train_test_split(data, test_size = 0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def content_to_wordlist(content):\n",
    "    conent_text = re.sub(\"[^а-яА-Яa-zA-Z]\",\" \", content)\n",
    "\n",
    "    words = conent_text.lower().split()\n",
    "\n",
    "    words = [w for w in words if not w in stops]\n",
    "\n",
    "    words = [wordnet_lemmatizer.lemmatize(w) for w in words]\n",
    "    return(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['content'] = train['content'].apply(content_to_wordlist)\n",
    "test['content'] = test['content'].apply(content_to_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_w2v = word2vec.Word2Vec(train['content'], vector_size=300, window=10, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_dict = {}\n",
    "for word in model_w2v.wv.index_to_key:\n",
    "    w2v_dict[word] = model_w2v.wv[word]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class mean_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(w2v_dict.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mean_vectorizer(w2v_dict).fit(train['content']).transform(train['content'])\n",
    "\n",
    "X_test = mean_vectorizer(w2v_dict).fit(train['content']).transform(test['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7405857740585774"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg  = LogisticRegression(solver='liblinear', C=9, random_state=42, n_jobs=-1)\n",
    "\n",
    "log_reg.fit(X_train, train['category'])\n",
    "\n",
    "log_reg_predict = log_reg.predict(X_test)\n",
    "\n",
    "accuracy_score(test['category'], log_reg_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7364016736401674"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(kernel='linear', C=5, random_state=42)\n",
    "\n",
    "svc.fit(X_train, train['category'])\n",
    "\n",
    "svc_predict = svc.predict(X_test)\n",
    "\n",
    "accuracy_score(test['category'], svc_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#пропишем класс выполняющий tfidf преобразование.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class tfidf_vectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(w2v_dict.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer(w2v_dict).fit(train['content']).transform(train['content'])\n",
    "\n",
    "X_test = tfidf_vectorizer(w2v_dict).fit(train['content']).transform(test['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6652719665271967"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg  = LogisticRegression(solver='liblinear', C=9, random_state=42, n_jobs=-1)\n",
    "\n",
    "log_reg.fit(X_train, train['category'])\n",
    "\n",
    "log_reg_predict = log_reg.predict(X_test)\n",
    "\n",
    "accuracy_score(test['category'], log_reg_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear', C=5, random_state=42)\n",
    "\n",
    "svc.fit(X_train, train['category'])\n",
    "\n",
    "svc_predict = svc.predict(X_test)\n",
    "\n",
    "accuracy_score(test['category'], svc_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
